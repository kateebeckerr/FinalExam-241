---
title: "EDS241: FINAL Template"
author: "Elliott Finn"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: 
  pdf_document:
    toc: false
    number_sections: yes
header-includes:
  \setlength{\parindent}{1em}
  \usepackage{float}
  \renewcommand{\thesubsection}{Question (\alph{subsection})}
--- 

Make sure to read through the setup in markdown. Remember to write out interpretations and report your results in writing/ table/plot forms.

``` {r setup, echo = FALSE, message = FALSE, warning = FALSE}
#Clean Environment
rm(list=ls())

# Setup your coding process in a way that works for you. 
# Ideally use projects to organize your scripts and outputs. 
# You all probably know more about this than us! 
# For this project, I would create a project with all your data and scripts. 
# I often store data on servers rather than my computer which is why I use the code you see below.

# I set an extension to retrieve data from a particular place (Google Drive/servers etc) 
# and projects to organize my scripts and outputs on my computer/github.

# here I am setting a path to where I stored the data for this assignment
data_wd <- "/Users/elliottfinn/Library/CloudStorage/GoogleDrive-elliottfinn@ucsb.edu/Shared drives/EDS241/Assignments/Assignment 2" 

# Example of how I use this Data Working Directory:
# data <- read_csv(paste0(data_wd,"/FILE_NAME.csv")) 
# This helps me download/manage my data from different places.

# set default chunk options
knitr::opts_chunk$set(fig.width = 4, fig.height = 3, 
                      echo = TRUE, message = FALSE, warning = FALSE)

# load packages
packages=c(
  # Necessary for Assignment 2
  "Match","plm", "tidyverse", "MatchIt", "RItools", "Hmisc", "lmtest", "estimatr",
  
  # You decide what works for you, these are the packages I use to display results 
  # they may not be the ones you use.
  "gridExtra", "stargazer", "kableExtra", 
  "purrr", "knitr", "broom",
  
  # Some Potentially useful packages from earlier examples
           "stargazer", "here","stringr", "janitor", 
           "cowplot", "ggplot2", "tinytex", "datasets", "tibble") # Used for Mock assignment

for (i in packages) {
  if (require(i,character.only=TRUE)==FALSE) {
    install.packages(i,repos='http://cran.us.r-project.org')
  }
  else {
    require(i,character.only=TRUE)
  }
}

# Disable scientific notation if you want
options(scipen=999)

```

# Part 1: RCTs, treatment ignorability (selection on observables), propensity scores _(15 points total)_

**Setup**

This exercise is inspired by Costello et al. 2008 article in science “Can Catch Shares Prevent Fisheries Collapse”, which we also discussed in class (lecture 5). “Inspired” means that the data final_fisheries_data.csv are synthetically generated to simplify things for our purposes. It contains the variables on 11,135 fisheries (only cross sectional, no time observations): These fisheries were either regulated by an Individual Transferable Quota (ITQ) for all years between 1990 and 2012 or in none of those years. Variables in the dataset include:

**The outcome and treatment variables are:**

\indent COLL_SHARE = share of years a fishery is collapsed between 1990 and 2012 (collapse defined as harvest being more than 10% below maximum recorded harvest).

\indent ITQ = dummy variable indicating ‘treatment’ with an ITQ (equal to 1 if the fishery has been regulated by an ITQ and 0 otherwise). 

**The control variables are:**

\indent MET1, MET2, ….MET6 = Dummy variables indicating to which Marine Ecosystem Type (MET) the fishery belongs to (coral reefs, kelp forests, seagrass meadows, open ocean, deep sea, mangrove forests). This type does not change over the relevant time period and does not depend on human influence.

\indent IND_SR = Index of species richness in 1980 with values between 0 and 100 indicating the biodiversity with respect to species in the fishery. Bounds of 0 and 100 are the lowest and highest observed values of species diversity across all fisheries in 1980, respectively.

\indent COMM_VAL = Commercial value of fisheries in 1980 in million US-$

The basic question of interest is “What is the average treatment effect of implementing an ITQ in the time period from 1990 to 2012 on the share of years with a collapse. It is likely that the probability a fishery is selected for an ITQ depends on the pre-treatment characteristics given. It is also quite likely that the pre-treatment characteristics have an effect on the share of collapse for each fishery, i.e. our outcome variable of interest.

```{r , include=TRUE}
rm(list=ls()) # clean environment

## Load Data

## Prepare Data
# How I setup my coding
# Keep project WD, set extension to get data from google drive 
data_wd <- "/Users/elliottfinn/Library/CloudStorage/GoogleDrive-elliottfinn@ucsb.edu/Shared drives/EDS241/Exam"

#LOAD DATA: final_fisheries_data.csv
fisheries <- read_csv("/Users/katebecker/Documents/Bren/Winter/241/FinalExam-241/data/final_fisheries_data.csv")


```
## Pretreatment Ecosystem Characteristic Comparison, Visual _(3 pts)_
(a) Compare the distributions of pre-treatment ecosystem characteristics (i.e. MET1, MET2, ,,, MET6) between the treated and the control groups by drawing back to back histograms [2 pts]. Write one sentence discussing the (dis)similarity between the two groups [1pt].


```{r , include=TRUE}
## Histograms comparing covariates
long_fish <- fisheries %>%
  pivot_longer(
    cols = starts_with("MET"),
    names_to = "met",
    values_to = "treatment",
    values_drop_na = TRUE
  )
```

```{r}
ggplot(long_fish, aes(x = met)) +
  geom_bar() +
  scale_x_discrete() +
  facet_wrap(~treatment) +
  labs(x = "Marine Ecosystem Type")
```


## Pretreatment Ecosystem Characteristic Comparison, Mean differences _3 pts)_
(b) Do a test on mean differences between the treated and control groups for the species richness index (IND_SR) and commercial value (COMM_VAL) variables. Interpret the results (estimated difference and significance) [2 pts] and make a conclusion regarding the similarity between the groups [1pt]. 


```{r , include=TRUE}
## Mean Differences (remember to use prop.test or t.test when applicable)
# use t test since continous var

#COMM_VAL
t.test(fisheries$COMM_VAL[fisheries$ITQ == 0],
        fisheries$COMM_VAL[fisheries$ITQ == 1])

#IND_SR
t.test(fisheries$IND_SR[fisheries$ITQ == 0],
        fisheries$IND_SR[fisheries$ITQ == 1])

```

## Treatment Ignorability _(1 pt)_
(c) Based on your results from (a) and (b), do you see a problem with just comparing the outcome variable means between treated and untreated fisheries? 

## Propensity Scores _(2 pts)_
(d) Estimate the propensity scores (probability of being treated) using a logit model, assume that all covariates are relevant and should be included in the estimation [0.5 pt]. Draw separate histograms (back to back) of the propensity scores for the treated and the untreated group [0.5 pt]. Comment on the overlap, do you have any concerns? Why/why not? [1]
```{r , include=TRUE}
## Propensity Score Estimates
xBalance(ITQ ~ MET1 + MET2 + MET3 + MET4 + MET5 + MET6 + IND_SR + COMM_VAL, data = fisheries,
         report=c("std.diffs","chisquare.test", "p.values"))

fish	<- glm(ITQ ~ MET1 + MET2 + MET3 + MET4 + MET5 + MET6 + IND_SR + COMM_VAL, data = fisheries,	family	= binomial())
summary(fish)	

fisheries$psvalue	<- predict(fish,	type	= "response")

histbackback(split(fisheries$psvalue,	fisheries$ITQ),	main= 
  "Propensity	score	before	matching",	xlab=c("control",	"treatment"))

```



## ATT with Nearest Neighbor Matching _(3 pts: 2 pt estimate, 1 pt interpretation)_
(e) Use the propensity scores from (c) to estimate the Average Treatment Effect on the Treated (ATT) with a nearest neighbor matching estimator. Interpret the result (just the size of the estimate)
```{r , include=TRUE}
## Nearest Neighbor Matching
nearest <- matchit(ITQ ~ MET1 + MET2 + MET3 + MET4 + MET5 + MET6 + IND_SR + COMM_VAL, data = fisheries,	method= "nearest",	ratio	= 1)
summary(nearest)
match_data	= match.data(nearest)

xBalance((ITQ ~ MET1 + MET2 + MET3 + MET4 + MET5 + MET6 + IND_SR + COMM_VAL, data = match_data,
         report=c("std.diffs","chisquare.test", "p.values")))
## Estimate ATT


ATT_data <- match_data %>%
  group_by(subclass) %>%
  mutate(diff = COLL_SHARE[ITQ==1]-COLL_SHARE[ITQ==0])

FT = sum(fisheries$ITQ)

sumdiff<-sum(ATT_data$diff)/2
ATT_m_nn = 1/FT * sumdiff
ATT_m_nn
```
## ATE with WLS _(3 pts: 1 pt estimate, 1 pt interpretation)_
(f) Estimate the Average Treatment Effect (ATE) using the weighted least squares on the full sample. Interpret the estimated size and conclude if it is significantly different from zero from a statistical perspective.
```{r , include=TRUE}
## WLS Matching
D = fisheries$ITQ
PS = fisheries$psvalue

fisheries$wgt = (D/PS + (1-D)/(1-PS))

fish_weight	<-lm(COLL_SHARE	~ ITQ,
             data = fisheries, weights = wgt)
summary(fish_weight)

## Estimate ATE
ATE_fish	<-lm(COLL_SHARE	~ ITQ + MET1 + MET2 + MET3 + MET4 + MET5 + MET6 + IND_SR + COMM_VAL, data = fisheries, weights = wgt)
summary(ATE_fish)
```


# Part 2 Difference in Difference Estimation _(10 points total + 3pts extra credit)_

\indent Here we return for a final time to the dataset from Gertler, Martinez, and Rubio-Codina (2012) and use a different way of estimating the effect of the Mexican conditional cash transfer on the value of animal holdings of recipients. We’ll use the panel data from assignment 2, where you have both the pre-program and post-program observations. See Template for dataset preparation instructions.

\indent **Data Preparation**

\indent *Note: You will need to install the packages plm and dplyr (included in template preamble). Again, you can find a description of the variables at the bottom of PDF and HERE.

Prepare Data: Load the new data (progresa_pre_1997.csv) and the follow-up data (progresa_post_1999.csv) into R. Note that we created a time denoting variable (with the same name, 'year') in BOTH datasets. Again, you will create a panel dataset by appending the data (i.e. binding the dataset row-wise together creating a single dataset). We want to examine the same outcome variable as before, value of family animal holdings (vani). You will use the full dataset for each estimate. NOTE: you should not change any NAs from the TREATED column in your analysis, as we expect that spillover was likely in this program. NAs will be excluded from your calculations/estimations.

```{r , include=TRUE, echo=FALSE}
rm(list=ls()) # clean environment

## Load/Prep Data
# How I setup my coding flow
# Keep project WD, set extension to get data from google drive 
data_wd <- "/Users/elliottfinn/Library/CloudStorage/GoogleDrive-elliottfinn@ucsb.edu/Shared drives/EDS241/Assignments/Assignment 2"

# Load 1997 and 1999 Progresa datasets
progressa_1997 <- read_csv("/Users/katebecker/Documents/Bren/Winter/241/FinalExam-241/data/progresa_pre_1997.csv")
progressa_1999 <- read_csv("/Users/katebecker/Documents/Bren/Winter/241/FinalExam-241/data/progresa_post_1999.csv")

####### CODE FIX FOR FINAL ######

### Append post to pre dataset 
# progresa_full <- rbind(progresa_pre_1997, progresa_post_1999) # same as original
#(note, you can keep NAs in the data- they'll be excluded from any estimates etc)
#progresa_full <- progresa_full %>%
#  group_by(hhid) %>% filter(n() == 2) %>%
#  ungroup()
# This removes all families lost to attrition, 
# in other words. Families who were treated/controls in the program, but did not get measured
# in the second year. This can happen for many reasons with observational data, you often
# lose participants as studies go on for longer periods of time.

rm(progresa_pre_1997, progresa_post_1999) # clean unused data

```
## DiD Estimator, ATE _(5 pts: 3 pts estimate, 2 pts interpretation)_
(a) Calculate the DiD estimator of the treatment effect (ATE) of the program on the value of animal holdings (vani)  “manually” i.e. based on group mean values without running a regression. Report and interpret the result (Note: no significance test or standard errors is possible, so you do not need to report these values).
```{r, include=TRUE}

## Estimate ATE with DiD estimator manually. 
# You will need to calculate various means to get this estimate

              
## Compute the Difference-in-Differences

```

## Difference in Difference using OLS _(5 pts)_
(b) Now set up an OLS-regression using group mean values to estimate the same ATE. Interpret the estimated treatment effect [3 pts]. Also interpret the coefficients on the time dummy and the group dummy variable (see interpretation done in class in lecture 9) [2 pts]. 

\indent **Hints:** You will need to create a new dataframe with a variety of dummy variables to do this. The R example provided with the DiD module (and/or the excel file) should help.

```{r, include=TRUE}

## Create a new data frame for OLS regression

## Run the OLS regression w/dummies

## Report OLS Model results Print the summary of the OLS model


```

# Extra Credit: ATE with OLS using full dataset _(3 pts: 2 pts estimate, 1 pt interpretation)_
(c) Estimate the ATE with an OLS-regression based on the original units as observations (i.e. not with group mean values, you will need to use the entire dataset). Even though the specification is the same as in the regression with the group mean values above, you’ll need to create new indicator variables for the treatment group and the post treatment time period as well as their interaction term. Verify that you get the same result as above. Now report also on the precision of the estimation and test whether the estimated coefficient is different from zero. 

```{r, include=TRUE}
## Create the dummy variables (you'll need 3)


## OLS regression


# Present Regressions in Table


```














